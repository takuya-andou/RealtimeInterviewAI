<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>カジュアル面談AI</title>
  <style>
    /* (既存のスタイルはそのまま) */
    body {
      font-family: "Hiragino Sans", "Yu Gothic", sans-serif;
      margin: 0;
      padding: 0;
      background: linear-gradient(120deg, #c9e9ff, #fce0fc);
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
      padding-bottom: 50px; /* 下部にスペースを追加 */
    }
    .container {
      background: rgba(255, 255, 255, 0.8);
      border-radius: 16px;
      box-shadow: 0 8px 20px rgba(0,0,0,0.1);
      width: 90%;
      max-width: 1400px;
      margin-top: 40px;
      padding: 30px;
      box-sizing: border-box;
    }
    h1 { margin-top: 0; text-align: center; font-weight: 600; color: #333; }
    button { padding: 12px 24px; margin: 8px; font-size: 16px; font-weight: bold; border: none; border-radius: 24px; cursor: pointer; transition: all 0.2s ease; background: #5768f3; color: #fff; box-shadow: 0 4px 14px rgba(0, 0, 0, 0.2); }
    button:disabled { opacity: 0.6; cursor: not-allowed; }
    button:hover:not(:disabled) { opacity: 0.8; }
    button.active-call:disabled { opacity: 1; background-color: #ccc; }
    .active-call { color: #f33; }
    audio { display: block; margin-top: 20px; width: 100%; }
    .config-section { margin-top: 20px; }
    .config-section label { display: block; margin-bottom: 8px; font-weight: bold; color: #555; }
    #prompt { width: calc(100% - 24px); padding: 12px; font-size: 16px; border: 1px solid #ccc; border-radius: 8px; resize: vertical; height: 120px; box-sizing: border-box; }
    #modelSelect, #voiceSelect { width: 100%; padding: 10px; font-size: 16px; border-radius: 8px; border: 1px solid #ccc; box-sizing: border-box; margin-top: 5px; }
    #hindsightSheet { width: calc(100% - 24px); padding: 12px; font-size: 16px; border: 1px solid #ccc; border-radius: 8px; resize: vertical; height: 120px; box-sizing: border-box; background-color: #ffffff; }

    /* --- Chat Area Styles --- */
    .conversation-area {
      margin-top: 25px;
      border: 1px solid #ddd;
      border-radius: 8px;
      padding: 15px;
      background-color: #f0f4f8; /* Light background for the area */
      height: 300px; /* Increased height */
      overflow-y: auto; /* Enable scrolling */
      box-shadow: inset 0 1px 3px rgba(0,0,0,0.05);
      display: flex;
      flex-direction: column; /* Stack messages vertically */
      gap: 10px; /* Space between messages */
    }
    .message {
      padding: 10px 15px;
      border-radius: 18px;
      max-width: 75%; /* Limit message width */
      line-height: 1.4;
      font-size: 0.95em;
      box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }
    .user-message {
      background-color: #dcf8c6; /* Light green for user */
      color: #333;
      align-self: flex-end; /* Align to the right */
      text-align: left; /* Keep text left-aligned within bubble */
      margin-left: auto; /* Push to the right */
    }
    .ai-message {
      background-color: #ffffff; /* White for AI */
      color: #333;
      align-self: flex-start; /* Align to the left */
      text-align: left;
      margin-right: auto; /* Push to the left */
    }
    /* Removed #summarizeButton styles */
    /* --- End Chat Area Styles --- */

    #summaryArea {
        margin-top: 25px;
        border: 1px solid #aec6cf; /* 少し目立つ色 */
        border-radius: 8px;
        padding: 15px;
        background-color: #e7f3f8; /* 薄い青系の背景 */
        display: none; /* 初期状態は非表示 */
    }
    #summaryArea h3 {
        margin-top: 0;
        color: #1f6d8f;
    }
    #summaryContent {
        white-space: pre-wrap; /* 改行を保持 */
        font-size: 0.95em;
        line-height: 1.5;
    }
    /* --- ここまで新しいスタイル --- */

    .footer-space { height: 40px; }
  </style>
</head>
<body>

  <div class="container">
    <h1>カジュアル面談AI</h1>

    <button id="startCall">開始</button>
    <button id="endCall" style="display: none;">通話終了</button>

    <audio id="remoteAudio" autoplay></audio>

    <div id="conversationArea" class="conversation-area"></div>

    <div id="summaryArea">
      <h3>会話の要約</h3>
      <p id="summaryContent">ここに要約が表示されます...</p>
    </div>

    <div id="promptSection" class="config-section">
      <label for="prompt">プロンプト:</label>
      <textarea id="prompt" placeholder="AIへの指示を入力してください"></textarea>
    </div>

    <div id="hindsightSection" class="config-section">
      <label for="hindsightSheet">ヒヤリング項目:</label>
      <textarea id="hindsightSheet" placeholder="最終的にまとめたい項目を記載してください">- 現在の業務内容・職場環境
- 転職を考えた動機・背景
- 希望するキャリアパスと目標
- これまでの経験・スキル、強み
- 希望する職種、業界、勤務地、給与などの条件
- 転職活動の進捗状況
- 転職エージェントに求めるサポート内容
- 転職に対する不安や懸念事項</textarea>
    </div>

    <div id="modelSelectSection" class="config-section">
      <label for="modelSelect">モデル:</label>
      <select id="modelSelect">
        <option value="gpt-4o-realtime-preview-2024-12-17">4o</option>
        <option value="gpt-4o-mini-realtime-preview-2024-12-17">4o-mini</option>
      </select>
    </div>

    <div id="voiceSelectSection" class="config-section">
        <label for="voiceSelect">音声:</label>
        <select id="voiceSelect">
          <option value="alloy">alloy</option>
          <option value="ash">ash</option>
          <option value="ballad">ballad</option>
          <option value="coral">coral</option>
          <option value="echo">echo</option>
          <option value="sage">sage</option>
          <option value="shimmer">shimmer</option>
          <option value="verse">verse</option>
        </select>
    </div>

    <div class="footer-space"></div>
  </div>

  <script>
    // --- DOM Element References ---
    const startButton = document.getElementById('startCall');
    const endButton = document.getElementById('endCall');
    // Removed summarizeButton reference
    const remoteAudio = document.getElementById('remoteAudio');
    const promptInput = document.getElementById('prompt');
    const hindsightSheet = document.getElementById('hindsightSheet'); // Added
    const modelSelect = document.getElementById('modelSelect');
    const voiceSelect = document.getElementById('voiceSelect');
    const conversationArea = document.getElementById('conversationArea'); // Added
    const summaryArea = document.getElementById('summaryArea');
    const summaryContent = document.getElementById('summaryContent');
    // Removed old transcript/indicator/box references


    // --- Default Values ---
    const DEFAULT_PROMPT = `あなたは転職エージェントとして振る舞う対話AIです。これから、求職者とのカジュアルな面談を通して、転職に関する背景や現状、希望条件、スキル、そして不安などを丁寧にヒアリングし、最適なサポートにつなげるための情報収集を行ってください。

【目的】
・求職者の現状や転職理由、将来のキャリアビジョン、スキル、希望条件、進捗状況、不安点などを把握する
・カジュアルでリラックスした雰囲気を保ちつつ、プロフェッショナルな視点で具体的な情報を引き出す

【質問項目】
1. 現在の状況と転職の動機  
　- 現在の業務内容、職場環境、転職を考えた背景や理由、現状で感じている課題などを尋ねる
2. 希望するキャリア像と目標  
　- 将来的にどのようなキャリアアップや成長を望んでいるか、具体的なビジョンを確認する
3. スキル・経験と強み  
　- これまでの経験、得意分野、転職先で活かせるスキルや実績、個人の強みについて詳しく聞く
4. 希望する職種や業界、勤務地、その他条件  
　- 興味のある業界・職種、希望勤務地、勤務形態、給与など、転職条件の優先順位を明確にする
5. 転職活動の進捗とサポートニーズ  
　- 現在進行中の転職活動の状況、応募中の企業や面接経験、エージェントに求めるサポート内容を確認する
6. 不安点や懸念事項  
　- 転職に対する不安や疑問、改善したい点について尋ね、求職者の悩みを整理する

【会話の方針】
・フレンドリーかつプロフェッショナルな口調で、リラックスした雰囲気を作る  
・オープンクエスチョンを用い、求職者が自由に意見を述べられるようにする  
・共感と肯定的なフィードバックを交えながら、丁寧にヒアリングを進める  
・求職者の回答に基づき柔軟に会話を進め、必要に応じて話題の深掘りや追加質問を行う  
・対話の中で、今後の転職活動や次のステップについても軽く触れる

【初回の対話例】
「はじめまして。転職エージェントの[あんどう]です。まず、現在のお仕事の内容や転職を考えたきっかけについてお聞かせいただけますか？」

上記の指針に沿って、対話を進めてください。
`;
    const DEFAULT_MODEL = "gpt-4o-mini-realtime-preview-2024-12-17";
    const DEFAULT_VOICE = "alloy";

    // --- State Variables ---
    let pc; // RTCPeerConnection
    let dataChannel;
    let localStream;
    let currentUserParagraph = null; // 現在のユーザー発話用 <p> 要素
    let currentAiParagraph = null;   // 現在のAI応答用 <p> 要素
    let lastSpeaker = null; // Track the last speaker ('user' or 'ai')


    // --- Initialization ---
    window.addEventListener('DOMContentLoaded', () => {
      promptInput.value = DEFAULT_PROMPT;
      modelSelect.value = DEFAULT_MODEL;
      voiceSelect.value = DEFAULT_VOICE;
    });

    // --- Logging Functions ---
    function log(message) { console.log(`[INFO] ${message}`); }
    function logError(message, error) { console.error(`[ERROR] ${message}`, error); }
    function logEvent(direction, eventData) { console.log(`[${direction}] Event: ${eventData.type}`, eventData); } // 詳細ログ

    // --- UI Update Functions ---
    // 指定された要素のスクロールを一番下に移動する関数
    function scrollToBottom(element) {
        // スクロール可能な親要素をスクロールさせる
        if (element && element.parentElement) {
             // 少し待ってからスクロールすると、要素のレンダリング後に実行されやすい
             setTimeout(() => {
                element.parentElement.scrollTop = element.parentElement.scrollHeight;
             }, 0);
        }
    }

    // 文字起こしと要約エリアをクリアする関数
    function clearTranscripts() {
        conversationArea.innerHTML = ''; // Updated
        currentUserParagraph = null; // 参照をリセット
        currentAiParagraph = null;  // 参照をリセット
        summaryArea.style.display = 'none'; // 要約エリアも隠す
        summaryContent.textContent = 'ここに要約が表示されます...';
        // Removed speaker indicator reset
    }

    // --- WebRTC and API Interaction ---
    async function startCall() {
      log('通話開始処理を開始します。');
      startButton.textContent = '開始中...';
      startButton.disabled = true;
      endButton.disabled = true;
      summaryRequested = false;
      clearTranscripts(); // 開始時にクリア

      if (pc) {
        log('既存のPeerConnectionを閉じています...');
        await endCallCleanup(); // 非同期である必要はないが念のため
      }

      try {
        // --- 1. Get Ephemeral Key ---
        log('エフェメラルAPIキーを取得中...');
        const userPrompt = promptInput.value || DEFAULT_PROMPT;
        const selectedModel = modelSelect.value;
        const selectedVoice = voiceSelect.value;

        // プロンプト（指示）を session.update で送る方式に変更も検討
        // ここでは初期接続時のパラメータとして voice と model を指定
        const tokenResponse = await fetch('/session', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          // 初期セッションパラメータ (プロンプトは後で session.update で送るか、response.create で指定)
          body: JSON.stringify({ voice: selectedVoice, model: selectedModel })
        });

        if (!tokenResponse.ok) {
          const errorData = await tokenResponse.json();
          throw new Error(`エフェメラルキー取得失敗: ${errorData.error?.message || tokenResponse.statusText}`);
        }
        const data = await tokenResponse.json();
        const EPHEMERAL_KEY = data.client_secret?.value;
        if (!EPHEMERAL_KEY) throw new Error("サーバーから有効なエフェメラルキーが返されませんでした。");
        log('エフェメラルAPIキーを取得しました。');

        // --- 2. Setup PeerConnection ---
        pc = new RTCPeerConnection();
        log('RTCPeerConnectionを作成しました。');

        pc.ontrack = event => {
          log('リモート音声ストリームを受信しました。');
          if (event.streams && event.streams[0]) {
            remoteAudio.srcObject = event.streams[0];
            remoteAudio.play().catch(e => logError('音声の自動再生に失敗しました:', e));
          } else { logError('受信したトラックにストリームが含まれていません。'); }
        };

        pc.oniceconnectionstatechange = () => log(`ICE接続状態: ${pc.iceConnectionState}`);
        pc.onconnectionstatechange = () => {
          log(`接続状態: ${pc.connectionState}`);
          if (pc.connectionState === 'connected') {
            log('WebRTC接続が確立しました。');
            startButton.textContent = '通話中';
            startButton.classList.add('active-call');
            startButton.disabled = true;
            endButton.style.display = 'inline-block';
            endButton.disabled = false;

            // 接続確立後にプロンプト（指示）を送信
             if (dataChannel && dataChannel.readyState === 'open') {
                 sendSessionUpdate(userPrompt);
             } else {
                 // dataChannelが開くまで少し待つか、onopenハンドラで送信する
                 const checkChannelOpen = setInterval(() => {
                     if (dataChannel && dataChannel.readyState === 'open') {
                         clearInterval(checkChannelOpen);
                         sendSessionUpdate(userPrompt);
                     }
                 }, 100);
                 // タイムアウト処理も追加するとより堅牢
             }

          } else if (['disconnected', 'failed', 'closed'].includes(pc.connectionState)) {
             logError(`WebRTC接続が切断または失敗しました (${pc.connectionState})。クリーンアップを実行します。`);
             endCallCleanup();
          }
        };

        // --- 3. Local Audio ---
        log('ローカル音声ストリームを取得中...');
        if (localStream) localStream.getTracks().forEach(track => track.stop());
        localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        localStream.getTracks().forEach(track => pc.addTrack(track, localStream));
        log('ローカル音声ストリームを追加しました。');

        // --- 4. Data Channel ---
        dataChannel = pc.createDataChannel('oai-events');
        dataChannel.onopen = () => {
            log('データチャネルが開きました。');
            // 接続状態が 'connected' になる前に開くことがあるので、
            // プロンプト送信は onconnectionstatechange で行う方が確実かもしれない
        };
        dataChannel.onclose = () => log('データチャネルが閉じました。');
        dataChannel.onerror = (error) => logError('データチャネルエラー:', error);
        dataChannel.onmessage = handleServerEvent; // メッセージハンドラを設定

        // --- 5. SDP Offer/Answer ---
        log('SDPオファーを作成中...');
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);
        log('ローカルSDPを設定しました。');

        const baseUrl = "https://api.openai.com/v1/realtime";
        // 接続URLにはモデルやボイスを含めず、session.updateで設定する方が良い場合もある
        const sdpResponse = await fetch(`${baseUrl}?model=${selectedModel}`, { // voiceは/sessionエンドポイントで設定済みの想定
          method: "POST", body: offer.sdp,
          headers: { Authorization: `Bearer ${EPHEMERAL_KEY}`, "Content-Type": "application/sdp" },
        });

        if (!sdpResponse.ok) {
          const errorText = await sdpResponse.text();
          throw new Error(`SDPアンサーの取得に失敗しました: ${sdpResponse.status} ${errorText}`);
        }
        const answerSDP = await sdpResponse.text();
        log('SDPアンサーを受信しました。');
        await pc.setRemoteDescription({ type: "answer", sdp: answerSDP });
        log('リモートSDPを設定しました。接続確立を待機します...');

      } catch (error) {
        logError('通話開始処理中にエラーが発生しました:', error);
        alert(`エラーが発生しました: ${error.message}`);
        endCallCleanup();
      }
    }

    // --- Function to send session update (e.g., instructions) ---
    function sendSessionUpdate(instructions) {
        if (!dataChannel || dataChannel.readyState !== 'open') {
            logError('データチャネルが利用できません。セッション更新を送信できません。');
            return;
        }
        try {
            const event = {
                type: "session.update",
                session: {
                    instructions: instructions
                },
            };
            log('セッション更新（指示）を送信します:', event);
            dataChannel.send(JSON.stringify(event));
        } catch (error) {
            logError('セッション更新イベントの送信に失敗しました:', error);
        }
    }


    // --- Server Event Handler ---
    function handleServerEvent(event) {
        try {
            const serverEvent = JSON.parse(event.data);
            logEvent('Server', serverEvent); // 詳細ログ

            switch (serverEvent.type) {
                // --- User Speech Handling ---
                case 'input_audio_buffer.speech_started':
                    log('User speech started');
                    // No action needed here now
                    break;
                case 'input_audio_buffer.speech_stopped':
                    log('User speech stopped');
                    // No action needed here now
                    break;

                // ユーザーの文字起こし完了イベント
                case 'conversation.item.input_audio_transcription.completed':
                    const userTranscript = serverEvent.transcript?.trim();
                    if (userTranscript) { // Only process if transcript is not empty/whitespace
                        // Check if the last speaker was also the user and the paragraph exists
                        if (lastSpeaker === 'user' && currentUserParagraph && conversationArea.contains(currentUserParagraph)) {
                            // Append to existing user message bubble
                            currentUserParagraph.textContent += ' ' + userTranscript;
                            log('Appended to existing user message.');
                        } else {
                            // Create a new user message bubble
                            currentUserParagraph = document.createElement('p');
                            currentUserParagraph.className = 'message user-message';
                            currentUserParagraph.textContent = userTranscript;
                            conversationArea.appendChild(currentUserParagraph);
                            log('Created new user message.');
                        }
                        lastSpeaker = 'user'; // Set last speaker to user
                        scrollToBottom(conversationArea);
                    } else {
                        log('Ignored empty user transcript.');
                    }
                    break;

                // ユーザーの文字起こし失敗イベント
                case 'conversation.item.input_audio_transcription.failed':
                    logError('ユーザーの音声文字起こしに失敗しました:', serverEvent);
                    break;

                // --- AI Response Handling ---
                case 'response.created':
                    // 要約応答の response.created は無視する
                    if (serverEvent.response?.metadata?.topic === 'summary') {
                        break;
                    }
                    log('AI response created');
                    // Create the paragraph element but DON'T append yet
                    currentAiParagraph = document.createElement('p');
                    currentAiParagraph.className = 'message ai-message';
                    currentAiParagraph.textContent = '';
                    lastSpeaker = 'ai'; // Set last speaker to AI
                    break;

                // AIの音声文字起こしデルタ
                case 'response.audio_transcript.delta':
                    // 要約応答のテキストデルタは無視する
                    if (serverEvent.response?.metadata?.topic === 'summary') {
                        break;
                    }
                    const aiDelta = serverEvent.delta;
                    if (aiDelta && aiDelta.trim()) { // Only process if delta is not empty/whitespace
                        // Ensure paragraph exists
                        if (!currentAiParagraph) {
                             // This case should ideally not happen if response.created is always received first
                            log('AI response.created was missed? Creating paragraph now.');
                            currentAiParagraph = document.createElement('p');
                            currentAiParagraph.className = 'message ai-message';
                            currentAiParagraph.textContent = '';
                            lastSpeaker = 'ai'; // Set speaker state if created here
                        }
                        // Append to DOM if it's not already there
                        if (!conversationArea.contains(currentAiParagraph)) {
                            conversationArea.appendChild(currentAiParagraph);
                            log('Appended AI message bubble to DOM.');
                        }
                        currentAiParagraph.textContent += aiDelta;
                        scrollToBottom(conversationArea); // Scroll when content is added
                    } else {
                        log('Ignored empty AI delta.');
                    }
                    break;

                // --- Summary Handling & Response Completion ---
                case 'response.done':
                    log(`Response done received. Metadata: ${JSON.stringify(serverEvent.response?.metadata)}`);

                    // Check if the completed AI paragraph exists and is empty, then remove it
                    if (currentAiParagraph && conversationArea.contains(currentAiParagraph) && !currentAiParagraph.textContent.trim()) {
                        log('Removing empty AI message bubble.');
                        currentAiParagraph.remove();
                    }

                    // Reset current AI paragraph reference ONLY
                    currentAiParagraph = null;
                    // lastSpeaker state remains 'ai' until the user speaks again
                    log('Normal AI response done.');
                    break;

                // --- Session & Other Events ---
                case 'session.created':
                    log('Session created:', serverEvent.session);
                    break;
                case 'session.updated':
                    log('Session updated:', serverEvent.session);
                    break;

                // --- Error Handling ---
                case 'error':
                case 'invalid_request_error':
                    logError('Server error event received:', serverEvent);
                    const errorCode = serverEvent.code || 'UNKNOWN_CODE';
                    const errorMsg = serverEvent.message || '不明なサーバーエラーが発生しました。';
                    const fullErrorMsg = `サーバーエラー (${errorCode}): ${errorMsg}`;
                    alert(fullErrorMsg); // ユーザーに通知

                    const errorP = document.createElement('p');
                    errorP.style.color = 'red';
                    errorP.style.fontWeight = 'bold';
                    errorP.textContent = `[エラー] ${errorMsg}`;
                    // エラーは会話エリアに表示する
                    conversationArea.appendChild(errorP); // Updated container
                    scrollToBottom(conversationArea); // Updated container

                    // fatalエラーなどで接続が維持できない場合がある
                    // 例: if (serverEvent.fatal || errorCode === 'session_expired' || ...)
                    const isFatal = serverEvent.fatal || ['session_expired', 'internal_server_error', 'authentication_error'].includes(errorCode); // 例
                    if (isFatal) {
                        log('Fatal error detected, cleaning up.');
                        endCallCleanup();
                    }

                    // Removed summaryRequested check
                    break;

                 // 他のイベントタイプも必要に応じて処理
                 default:
                     logEvent('Server (Unhandled)', serverEvent); // 未処理イベントはログ出力
                     break;
            }
        } catch (err) {
            logError('Failed to parse or handle server message:', err);
            console.log('Raw message data:', event.data); // エラー時の生データを表示
        }
    }

    // --- End Call Initiator (Now includes summarization) ---
    async function endCall() {
        log('通話終了処理（要約作成とクリーンアップ）を開始します。');
        endButton.textContent = '要約作成中...';
        endButton.disabled = true;
        summaryArea.style.display = 'none'; // Hide previous summary
        summaryContent.textContent = '要約を作成しています...';

        // First, clean up the WebRTC connection
        // We do this first to stop audio streams immediately
        if (dataChannel) {
            try {
                if (dataChannel.readyState === 'open') dataChannel.close();
            } catch (e) { logError('データチャネルクローズエラー:', e); }
            dataChannel = null;
        }
        if (pc) {
            try {
                pc.close();
            } catch (e) { logError('PeerConnectionクローズエラー:', e); }
            pc = null;
        }
        if (localStream) {
            localStream.getTracks().forEach(track => track.stop());
            localStream = null;
        }
        if(remoteAudio.srcObject){
            remoteAudio.pause();
            remoteAudio.srcObject = null;
        }
        log('WebRTC接続をクリーンアップしました。要約を作成します...');


        // Now, attempt summarization
        try {
            // Collect transcript text
            const messages = conversationArea.querySelectorAll('.message');
            let transcriptText = '';
            messages.forEach(msg => {
                const speaker = msg.classList.contains('user-message') ? 'User:' : 'AI:';
                transcriptText += `${speaker} ${msg.textContent}\n`;
            });

            if (!transcriptText.trim()) {
                alert('会話の記録がないため、要約を作成できません。');
                // No summary to create, just finish cleanup UI
                endCallCleanupUI();
                return;
            }

            const hearingItemsText = hindsightSheet.value.trim(); // Get hearing items text
            log('Sending transcript and hearing items to backend for summarization.');
            // 2. Send to backend endpoint
            const response = await fetch('/summarize', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    transcript: transcriptText.trim(),
                    hearingItems: hearingItemsText // Include hearing items
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(errorData.error || `サーバーエラー: ${response.status}`);
            }

            const result = await response.json();

            // 3. Display summary
            summaryContent.textContent = result.summary || '要約の取得に失敗しました。';
            summaryArea.style.display = 'block';
            scrollToBottom(summaryArea); // Scroll to show summary
            log('要約を表示しました。');

        } catch (error) {
            logError('要約作成中にエラーが発生しました:', error);
            alert(`要約の作成に失敗しました: ${error.message}`);
            summaryContent.textContent = '要約の作成に失敗しました。';
            summaryArea.style.display = 'block'; // Show error message in summary area
        } finally {
            // Reset UI after summarization attempt
            endCallCleanupUI();
        }
    }


    // --- UI Cleanup Function (Separated from WebRTC cleanup) ---
    function endCallCleanupUI() {
      // Reset UI States
      startButton.textContent = '開始';
      startButton.classList.remove('active-call');
      startButton.disabled = false;
      startButton.style.display = 'inline-block'; // Show start button again
      endButton.style.display = 'none';
      endButton.textContent = '通話終了';
      endButton.disabled = true;
      // Removed summarizeButton display logic
      // summaryArea display is handled by endCall
      currentUserParagraph = null;
      currentAiParagraph = null;
      lastSpeaker = null; // Reset speaker state

      log('UIクリーンアップが完了しました。');
    }


    // --- Event Listeners ---
    startButton.addEventListener('click', startCall);
    endButton.addEventListener('click', endCall); // Now triggers summary + cleanup
    // Removed summarizeButton listener

    window.addEventListener('beforeunload', (event) => {
      // 通話中にページを離れようとした場合
      if (pc && pc.connectionState !== 'closed' && pc.connectionState !== 'failed') {
        log('ページ離脱前にクリーンアップを実行します。');
        // 即時クリーンアップ（要約などは行わない）
        // 非同期処理は間に合わない可能性があるので同期的にできることを行う
        if (dataChannel && dataChannel.readyState === 'open') dataChannel.close();
        if (pc) pc.close();
        if (localStream) localStream.getTracks().forEach(track => track.stop());
        // ユーザーに確認メッセージを表示することもできる (ただし、最近のブラウザでは制限あり)
        // event.preventDefault();
        // event.returnValue = '通話中にページを離れようとしています。よろしいですか？';
      }
    });

  </script>
</body>
</html>
